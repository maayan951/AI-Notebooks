{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Random Forest vs Extra Trees"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Overview\n","Scikit-Learn is packed with amazing algoritms which serves a wide variety of purposes. Needless to say one algoritm does not fit all datasets. Each algoritm comes with its pros and cons and our role is to keep experimenting and pick the algorithm which serve our needs.\n","In this notebook, we will look at a special case of the famous Random Forest ensemble called Extremely Randomized Trees or Extra Trees and will see how can this ensemble become helpful in our projects. We will also compare it with the traditional Random Forest in terms of computational complexity. To get started lets first look at a basic Decision Tree.\n","\n","<img src=\"https://i.stack.imgur.com/Q18mk.png\">"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Decision Tree\n","As the name suggest, Decision Tree is a tree with levels (or a depth), each level has nodes and each node takes a decision based on a certain threshold. Each node splits the dataset into two which helps in classifying any instance into categories and collectively into target classes. Look at the following figure for a better understanding (<a href=\"https://towardsdatascience.com/an-introduction-to-decision-trees-with-python-and-scikit-learn-1a5ba6fc204f\">Source</a>)\n","\n","<img src=\"https://miro.medium.com/max/1400/1*fGX0_gacojVa6-njlCrWZw.png\" width=600 height=400/>\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Here, the first node divides the dataset by sex, at the second level the dataset is divided by the features Age and Pclass and so on. The important thing to note here is that the threshold to divide the tree at a particular node is selected by the algorithm on its own. It calculates this threshold by analysing the feature and threshold that will give the least gini impurity. In simple terms, if we have a dataset which classifies students into pass or fail and we have only one feature i.e Marks, the decision tree will find the passing score(threshold) on its own and train a model. The next time we enter a student's marks, the algorithm classifies the student into pass or fail using the computed passing score."]},{"cell_type":"markdown","metadata":{},"source":["## 3. Random Forest\n","Going from Decision Tree to Random Forest is simple. Instead of a single tree on an empty land(let's say apple tree), imagine a forest with 1000s of trees(but not every tree is an apple tree). Each tree bear a slightly different fruit with different taste, colour, or altogether a different fruit. This variety usually makes the harvest of the forest much more high-yielding. \n","\n","The Decision Tree takes the whole dataset and creates a tree. While Random Forest produces many trees but each tree is not shown the entire training dataset, it is shown only a part of the dataset and then it predicts the class or value. Later, all predicted values of all trees are cumulated to make the final prediction. \n","\n","The random partial data is shown to each tree to bring diversity. Imagine if all the trees are shown the entire dataset, then each tree will bear the same identical apple fruit, which will add no additional value than just using a single Decision Tree algoritm. Look at the following image for better understanding (<a href=\"https://towardsdatascience.com/random-forest-classification-and-its-implementation-d5d840dbead0\">Source</a>).\n","\n","<img src=\"https://miro.medium.com/max/1148/0*a8KgF1IINziv7KIQ.png\" />"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Extra Trees\n","The \"Random\" in Random Forest is bceause we are using a random subset of the dataset. But what if instead of choosing the best possible threshold for each tree at each node, we simply choose a random threshold too. If you remember the example taken in Decision Trees, imagine the algorithm is not bothered to compute the passing score in the first attempt, instead it takes a random marks value which divides the dataset(let that value be 90). Now at the first level, the students are divided into two categories (greater than and less than 90). If a student scores more than 90 he will be declared as pass, but if he scores less than 90, then the algorithm will find another random threshold (between 0 and 90) and categorize the remaining instances further. This algorithm too will eventually create an accurate model but it will have far more number of levels and nodes than a Decision Tree. However, the random nature of choosing the threshold value will make it much more faster.\n","\n","<img src=\"https://www.researchgate.net/profile/Navoneel-Chakrabarty/publication/341967355/figure/fig1/AS:901875410948097@1592035262515/Visual-Representation-of-Extra-Trees-Classifier.ppm\">"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Implementation\n","\n","Lets create our Extra Trees model!  \n","### 5.1. Importing dataset \n","We will be using breast cancer data present in Scikit-Learn datasets. This dataset has 30 features and 569 instances."]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","from sklearn.datasets import load_breast_cancer"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["data = load_breast_cancer()"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean radius</th>\n","      <th>mean texture</th>\n","      <th>mean perimeter</th>\n","      <th>mean area</th>\n","      <th>mean smoothness</th>\n","      <th>mean compactness</th>\n","      <th>mean concavity</th>\n","      <th>mean concave points</th>\n","      <th>mean symmetry</th>\n","      <th>mean fractal dimension</th>\n","      <th>...</th>\n","      <th>worst radius</th>\n","      <th>worst texture</th>\n","      <th>worst perimeter</th>\n","      <th>worst area</th>\n","      <th>worst smoothness</th>\n","      <th>worst compactness</th>\n","      <th>worst concavity</th>\n","      <th>worst concave points</th>\n","      <th>worst symmetry</th>\n","      <th>worst fractal dimension</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17.99</td>\n","      <td>10.38</td>\n","      <td>122.80</td>\n","      <td>1001.0</td>\n","      <td>0.11840</td>\n","      <td>0.27760</td>\n","      <td>0.30010</td>\n","      <td>0.14710</td>\n","      <td>0.2419</td>\n","      <td>0.07871</td>\n","      <td>...</td>\n","      <td>25.380</td>\n","      <td>17.33</td>\n","      <td>184.60</td>\n","      <td>2019.0</td>\n","      <td>0.16220</td>\n","      <td>0.66560</td>\n","      <td>0.7119</td>\n","      <td>0.2654</td>\n","      <td>0.4601</td>\n","      <td>0.11890</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20.57</td>\n","      <td>17.77</td>\n","      <td>132.90</td>\n","      <td>1326.0</td>\n","      <td>0.08474</td>\n","      <td>0.07864</td>\n","      <td>0.08690</td>\n","      <td>0.07017</td>\n","      <td>0.1812</td>\n","      <td>0.05667</td>\n","      <td>...</td>\n","      <td>24.990</td>\n","      <td>23.41</td>\n","      <td>158.80</td>\n","      <td>1956.0</td>\n","      <td>0.12380</td>\n","      <td>0.18660</td>\n","      <td>0.2416</td>\n","      <td>0.1860</td>\n","      <td>0.2750</td>\n","      <td>0.08902</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>19.69</td>\n","      <td>21.25</td>\n","      <td>130.00</td>\n","      <td>1203.0</td>\n","      <td>0.10960</td>\n","      <td>0.15990</td>\n","      <td>0.19740</td>\n","      <td>0.12790</td>\n","      <td>0.2069</td>\n","      <td>0.05999</td>\n","      <td>...</td>\n","      <td>23.570</td>\n","      <td>25.53</td>\n","      <td>152.50</td>\n","      <td>1709.0</td>\n","      <td>0.14440</td>\n","      <td>0.42450</td>\n","      <td>0.4504</td>\n","      <td>0.2430</td>\n","      <td>0.3613</td>\n","      <td>0.08758</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.42</td>\n","      <td>20.38</td>\n","      <td>77.58</td>\n","      <td>386.1</td>\n","      <td>0.14250</td>\n","      <td>0.28390</td>\n","      <td>0.24140</td>\n","      <td>0.10520</td>\n","      <td>0.2597</td>\n","      <td>0.09744</td>\n","      <td>...</td>\n","      <td>14.910</td>\n","      <td>26.50</td>\n","      <td>98.87</td>\n","      <td>567.7</td>\n","      <td>0.20980</td>\n","      <td>0.86630</td>\n","      <td>0.6869</td>\n","      <td>0.2575</td>\n","      <td>0.6638</td>\n","      <td>0.17300</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>20.29</td>\n","      <td>14.34</td>\n","      <td>135.10</td>\n","      <td>1297.0</td>\n","      <td>0.10030</td>\n","      <td>0.13280</td>\n","      <td>0.19800</td>\n","      <td>0.10430</td>\n","      <td>0.1809</td>\n","      <td>0.05883</td>\n","      <td>...</td>\n","      <td>22.540</td>\n","      <td>16.67</td>\n","      <td>152.20</td>\n","      <td>1575.0</td>\n","      <td>0.13740</td>\n","      <td>0.20500</td>\n","      <td>0.4000</td>\n","      <td>0.1625</td>\n","      <td>0.2364</td>\n","      <td>0.07678</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>564</th>\n","      <td>21.56</td>\n","      <td>22.39</td>\n","      <td>142.00</td>\n","      <td>1479.0</td>\n","      <td>0.11100</td>\n","      <td>0.11590</td>\n","      <td>0.24390</td>\n","      <td>0.13890</td>\n","      <td>0.1726</td>\n","      <td>0.05623</td>\n","      <td>...</td>\n","      <td>25.450</td>\n","      <td>26.40</td>\n","      <td>166.10</td>\n","      <td>2027.0</td>\n","      <td>0.14100</td>\n","      <td>0.21130</td>\n","      <td>0.4107</td>\n","      <td>0.2216</td>\n","      <td>0.2060</td>\n","      <td>0.07115</td>\n","    </tr>\n","    <tr>\n","      <th>565</th>\n","      <td>20.13</td>\n","      <td>28.25</td>\n","      <td>131.20</td>\n","      <td>1261.0</td>\n","      <td>0.09780</td>\n","      <td>0.10340</td>\n","      <td>0.14400</td>\n","      <td>0.09791</td>\n","      <td>0.1752</td>\n","      <td>0.05533</td>\n","      <td>...</td>\n","      <td>23.690</td>\n","      <td>38.25</td>\n","      <td>155.00</td>\n","      <td>1731.0</td>\n","      <td>0.11660</td>\n","      <td>0.19220</td>\n","      <td>0.3215</td>\n","      <td>0.1628</td>\n","      <td>0.2572</td>\n","      <td>0.06637</td>\n","    </tr>\n","    <tr>\n","      <th>566</th>\n","      <td>16.60</td>\n","      <td>28.08</td>\n","      <td>108.30</td>\n","      <td>858.1</td>\n","      <td>0.08455</td>\n","      <td>0.10230</td>\n","      <td>0.09251</td>\n","      <td>0.05302</td>\n","      <td>0.1590</td>\n","      <td>0.05648</td>\n","      <td>...</td>\n","      <td>18.980</td>\n","      <td>34.12</td>\n","      <td>126.70</td>\n","      <td>1124.0</td>\n","      <td>0.11390</td>\n","      <td>0.30940</td>\n","      <td>0.3403</td>\n","      <td>0.1418</td>\n","      <td>0.2218</td>\n","      <td>0.07820</td>\n","    </tr>\n","    <tr>\n","      <th>567</th>\n","      <td>20.60</td>\n","      <td>29.33</td>\n","      <td>140.10</td>\n","      <td>1265.0</td>\n","      <td>0.11780</td>\n","      <td>0.27700</td>\n","      <td>0.35140</td>\n","      <td>0.15200</td>\n","      <td>0.2397</td>\n","      <td>0.07016</td>\n","      <td>...</td>\n","      <td>25.740</td>\n","      <td>39.42</td>\n","      <td>184.60</td>\n","      <td>1821.0</td>\n","      <td>0.16500</td>\n","      <td>0.86810</td>\n","      <td>0.9387</td>\n","      <td>0.2650</td>\n","      <td>0.4087</td>\n","      <td>0.12400</td>\n","    </tr>\n","    <tr>\n","      <th>568</th>\n","      <td>7.76</td>\n","      <td>24.54</td>\n","      <td>47.92</td>\n","      <td>181.0</td>\n","      <td>0.05263</td>\n","      <td>0.04362</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.1587</td>\n","      <td>0.05884</td>\n","      <td>...</td>\n","      <td>9.456</td>\n","      <td>30.37</td>\n","      <td>59.16</td>\n","      <td>268.6</td>\n","      <td>0.08996</td>\n","      <td>0.06444</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.2871</td>\n","      <td>0.07039</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>569 rows × 30 columns</p>\n","</div>"],"text/plain":["     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n","0          17.99         10.38          122.80     1001.0          0.11840   \n","1          20.57         17.77          132.90     1326.0          0.08474   \n","2          19.69         21.25          130.00     1203.0          0.10960   \n","3          11.42         20.38           77.58      386.1          0.14250   \n","4          20.29         14.34          135.10     1297.0          0.10030   \n","..           ...           ...             ...        ...              ...   \n","564        21.56         22.39          142.00     1479.0          0.11100   \n","565        20.13         28.25          131.20     1261.0          0.09780   \n","566        16.60         28.08          108.30      858.1          0.08455   \n","567        20.60         29.33          140.10     1265.0          0.11780   \n","568         7.76         24.54           47.92      181.0          0.05263   \n","\n","     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n","0             0.27760         0.30010              0.14710         0.2419   \n","1             0.07864         0.08690              0.07017         0.1812   \n","2             0.15990         0.19740              0.12790         0.2069   \n","3             0.28390         0.24140              0.10520         0.2597   \n","4             0.13280         0.19800              0.10430         0.1809   \n","..                ...             ...                  ...            ...   \n","564           0.11590         0.24390              0.13890         0.1726   \n","565           0.10340         0.14400              0.09791         0.1752   \n","566           0.10230         0.09251              0.05302         0.1590   \n","567           0.27700         0.35140              0.15200         0.2397   \n","568           0.04362         0.00000              0.00000         0.1587   \n","\n","     mean fractal dimension  ...  worst radius  worst texture  \\\n","0                   0.07871  ...        25.380          17.33   \n","1                   0.05667  ...        24.990          23.41   \n","2                   0.05999  ...        23.570          25.53   \n","3                   0.09744  ...        14.910          26.50   \n","4                   0.05883  ...        22.540          16.67   \n","..                      ...  ...           ...            ...   \n","564                 0.05623  ...        25.450          26.40   \n","565                 0.05533  ...        23.690          38.25   \n","566                 0.05648  ...        18.980          34.12   \n","567                 0.07016  ...        25.740          39.42   \n","568                 0.05884  ...         9.456          30.37   \n","\n","     worst perimeter  worst area  worst smoothness  worst compactness  \\\n","0             184.60      2019.0           0.16220            0.66560   \n","1             158.80      1956.0           0.12380            0.18660   \n","2             152.50      1709.0           0.14440            0.42450   \n","3              98.87       567.7           0.20980            0.86630   \n","4             152.20      1575.0           0.13740            0.20500   \n","..               ...         ...               ...                ...   \n","564           166.10      2027.0           0.14100            0.21130   \n","565           155.00      1731.0           0.11660            0.19220   \n","566           126.70      1124.0           0.11390            0.30940   \n","567           184.60      1821.0           0.16500            0.86810   \n","568            59.16       268.6           0.08996            0.06444   \n","\n","     worst concavity  worst concave points  worst symmetry  \\\n","0             0.7119                0.2654          0.4601   \n","1             0.2416                0.1860          0.2750   \n","2             0.4504                0.2430          0.3613   \n","3             0.6869                0.2575          0.6638   \n","4             0.4000                0.1625          0.2364   \n","..               ...                   ...             ...   \n","564           0.4107                0.2216          0.2060   \n","565           0.3215                0.1628          0.2572   \n","566           0.3403                0.1418          0.2218   \n","567           0.9387                0.2650          0.4087   \n","568           0.0000                0.0000          0.2871   \n","\n","     worst fractal dimension  \n","0                    0.11890  \n","1                    0.08902  \n","2                    0.08758  \n","3                    0.17300  \n","4                    0.07678  \n","..                       ...  \n","564                  0.07115  \n","565                  0.06637  \n","566                  0.07820  \n","567                  0.12400  \n","568                  0.07039  \n","\n","[569 rows x 30 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data_X = pd.DataFrame(data[\"data\"], columns=data['feature_names'])\n","data_X"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n","       0, 0, 0, 0, 0, 0, 0, 0])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["data_Y = data[\"target\"]\n","data_Y[:30]"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["(569, 30)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["data_X.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["(569,)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["data_Y.shape"]},{"cell_type":"markdown","metadata":{},"source":["### 5.2. Creating a basic Extra Trees Classifier"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.model_selection import cross_val_score"]},{"cell_type":"markdown","metadata":{},"source":["We will use 5-fold cross validation and take its average to compute the accuracy of model."]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The avergae Cross validation score is  0.9701288619779538\n"]}],"source":["etc = ExtraTreesClassifier(random_state=0)\n","cv_score = cross_val_score(etc, data_X, data_Y, cv=5).mean()\n","print(\"The avergae Cross validation score is \", cv_score)"]},{"cell_type":"markdown","metadata":{},"source":["Wow! Without any hyperparameter tuning we achieved a validation score of 97% The model is working pretty well on its own, still lets look at some parameters that we can tweak."]},{"cell_type":"markdown","metadata":{},"source":["## 6. Hypertuning\n","\n","Here are some of the popular tuning parameters provided by Scikit-Learn :- \n","1. <b>n_estimators</b> : The number of trees in a forest. Default = 100\n","2. <b>max_depth</b> : The maximum depth of the forest. Default = None i.e the tree will continue to expand untill all nodes are pure.\n","3. <b>min_samples_split</b> : The minimum number of samples required to split a node. Default = 2\n","4. <b>min_samples_leaf</b> : The minimum number of samples required to be at leaf node. This parameter becomes important specially if max_depth=None. Default = 1.\n","\n","Check out other parameters in Scikit-Learn's documentation(<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html?highlight=extra#sklearn.ensemble.ExtraTreesClassifier\">here</a>).\n","\n","Here, we will hypertune n_estimators and max_depth by using GridSerchCV"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["GridSearchCV(cv=5, error_score=nan,\n","             estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,\n","                                            class_weight=None, criterion='gini',\n","                                            max_depth=None, max_features='auto',\n","                                            max_leaf_nodes=None,\n","                                            max_samples=None,\n","                                            min_impurity_decrease=0.0,\n","                                            min_impurity_split=None,\n","                                            min_samples_leaf=1,\n","                                            min_samples_split=2,\n","                                            min_weight_fraction_leaf=0.0,\n","                                            n_estimators=100, n_jobs=None,\n","                                            oob_score=False, random_state=None,\n","                                            verbose=0, warm_start=False),\n","             iid='deprecated', n_jobs=None,\n","             param_grid={'max_depth': [5, 10, 50, 100, 500],\n","                         'n_estimators': [50, 100, 500, 1000]},\n","             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n","             scoring=None, verbose=0)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["parameters = {'n_estimators':[50, 100, 500, 1000], 'max_depth':[5, 10, 50, 100, 500]}\n","etc = ExtraTreesClassifier()\n","clf = GridSearchCV(etc, parameters, cv=5)\n","clf.fit(data_X, data_Y)"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n","                     criterion='gini', max_depth=500, max_features='auto',\n","                     max_leaf_nodes=None, max_samples=None,\n","                     min_impurity_decrease=0.0, min_impurity_split=None,\n","                     min_samples_leaf=1, min_samples_split=2,\n","                     min_weight_fraction_leaf=0.0, n_estimators=100,\n","                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n","                     warm_start=False)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["clf.best_estimator_"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["0.9736376339077782"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["clf.best_score_"]},{"cell_type":"markdown","metadata":{},"source":["Turns out that max_depth=500 and n_estimators=50 gives the best score of 97.3% which is a slight improvement over default model."]},{"cell_type":"markdown","metadata":{},"source":["## 7. Comparison\n","Finally, lets see how the Extra Trees algorithm compares with Decision Trees and Random Forest in terms of Accuracy and Time. We will use default model parameters instead of hypertuning."]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.model_selection import cross_val_score\n","import time"]},{"cell_type":"markdown","metadata":{},"source":["### 7.1. Decision Tree"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average Cross Validation Score =  0.9173730787144851\n","--- 0.060630083084106445 seconds ---\n"]}],"source":["start_time = time.time()\n","\n","dtc = DecisionTreeClassifier(random_state=0)\n","cv_score = cross_val_score(dtc, data_X, data_Y, cv=5).mean()\n","print(\"Average Cross Validation Score = \", cv_score)\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"]},{"cell_type":"markdown","metadata":{},"source":["This algoritm is incredibly fast and took only 0.06 seconds! Reason being that it just trains a single Tree and predicts. Validation score is 91.7%"]},{"cell_type":"markdown","metadata":{},"source":["### 7.2. Random Forest"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average Cross Validation Score =  0.9631113181183046\n","--- 1.5104308128356934 seconds ---\n"]}],"source":["start_time = time.time()\n","\n","rfc = RandomForestClassifier(random_state=0)\n","cv_score = cross_val_score(rfc, data_X, data_Y, cv=5).mean()\n","print(\"Average Cross Validation Score = \", cv_score)\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"]},{"cell_type":"markdown","metadata":{},"source":["Random Forest took 1.52 seconds which is 25 times slower than a single Decision Tree which is obvious due to more number of estimators(trees). Validation score is 96.3%"]},{"cell_type":"markdown","metadata":{},"source":["### 7.3. Extra Trees"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average Cross Validation Score =  0.9701288619779538\n","--- 0.9814736843109131 seconds ---\n"]}],"source":["start_time = time.time()\n","\n","etc = ExtraTreesClassifier(random_state=0)\n","cv_score = cross_val_score(etc, data_X, data_Y, cv=5).mean()\n","print(\"Average Cross Validation Score = \", cv_score)\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"]},{"cell_type":"markdown","metadata":{},"source":["Wow! With the same default parameters, validation score of Extra Trees is very close(slightly better) than that of Random Forest i.e 97%. But the important thing to note here is the time complexity, it trains the model in somewhat 60% time as that taken by Random Forest."]},{"cell_type":"markdown","metadata":{},"source":["## 8. Conclusion\n","Its hard to pin out which algorithm is better in terms of accuracy and would depend on testing. However, Extremely Randomized Trees are surely faster than Random Forest due to the random nature of picking up thresholds. Extra Trees can become a very useful algorithm if your dataset is huge and you want to quickly run a Decision Tree ensemble and check how your model performs on the dataset. Check out this link to know the exact time complexities of different machine learning algoritms(<a href=\"https://www.thekerneltrip.com/machine/learning/computational-complexity-learning-algorithms/\">Link</a>)."]},{"cell_type":"markdown","metadata":{},"source":["### Please upvote this notebook if you found it to be useful!"]},{"cell_type":"markdown","metadata":{},"source":["Reference - Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow by Aurélien Géron"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"405d13d09847d50ecc6343eff00cc7200d674373355acea1c385012d3714c1ef"}}},"nbformat":4,"nbformat_minor":4}
